# -*- coding: utf-8 -*-
"""aula2-treinarModeloRegressao.ipynb .

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zuPDrZ-qyBooEhsdjYbIEsiDsBeb_V5F

O aprendizado de máquina supervisionado envolve treinar um modelo para operar em um conjunto de recursos e prever um rótulo usando um conjunto de dados que inclui alguns valores de rótulo já conhecidos. O processo de treinamento ajusta os recursos aos rótulos conhecidos para definir uma função geral que pode ser aplicada a novos recursos para os quais os rótulos são desconhecidos e prevê-los. Você pode pensar nessa função assim, em que y representa o rótulo que queremos prever e x representa os recursos que o modelo usa para prever:

y=f(x)

Na maioria dos casos, x é na verdade um vetor que consiste em vários valores de recurso, então, para ser um pouco mais preciso, a função pode ser expressa assim:

y=f([x 1​,x 2​,x 3​,...])

O objetivo de treinar o modelo é encontrar uma função que execute algum tipo de cálculo nos valores x que produza o resultado y. Fazemos isso aplicando um algoritmo de aprendizado de máquina que tenta ajustar os valores x a um cálculo que produza y com razoável precisão para todos os casos no conjunto de dados de treinamento.

Existem muitos algoritmos de aprendizado de máquina para aprendizado supervisionado e podemos dividi-los amplamente em dois tipos:

Algoritmos de regressão: algoritmos que prevêem um valor y que é um valor numérico, como o preço de uma casa ou o número de transações de vendas. Algoritmos de classificação: algoritmos que prevêem a qual categoria, ou classe, uma observação pertence. O valor y em um modelo de classificação é um vetor de valores de probabilidade entre 0 e 1, um para cada classe, indicando a probabilidade da observação pertencer a cada classe. Neste notebook, vamos nos concentrar na regressão, usando um exemplo baseado em um estudo real no qual dados para um esquema de compartilhamento de bicicletas foram coletados e usados para prever o número de aluguéis com base na sazonalidade e nas condições climáticas. Usaremos uma versão simplificada do conjunto de dados desse estudo.

A primeira etapa em qualquer projeto de aprendizado de máquina é explorar os dados que você usará para treinar um modelo. O objetivo dessa exploração é tentar entender as relações entre seus atributos; em particular, qualquer correlação aparente entre os recursos e o rótulo que seu modelo tentará prever.
"""

import pandas as pd

# load the training dataset
!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/daily-bike-share.csv
bike_data = pd.read_csv('daily-bike-share.csv')
bike_data.head()

"""The data consists of the following columns:

instant: A unique row identifier
dteday: The date on which the data was observed; in this case, the data was collected daily, so there's one row per date.
season: A numerically encoded value indicating the season (1:winter, 2:spring, 3:summer, 4:fall)
yr: The year of the study in which the observation was made (the study took place over two years: year 0 represents 2011, and year 1 represents 2012)
mnth: The calendar month in which the observation was made (1:January ... 12:December)
holiday: A binary value indicating whether or not the observation was made on a public holiday)
weekday: The day of the week on which the observation was made (0:Sunday ... 6:Saturday)
workingday: A binary value indicating whether or not the day is a working day (not a weekend or holiday)
weathersit: A categorical value indicating the weather situation (1:clear, 2:mist/cloud, 3:light rain/snow, 4:heavy rain/hail/snow/fog)
temp: The temperature in celsius (normalized)
atemp: The apparent ("feels-like") temperature in celsius (normalized)
hum: The humidity level (normalized)
windspeed: The windspeed (normalized)
rentals: The number of bicycle rentals recorded.
In this dataset, rentals represents the label (the y value) we must train our model to predict. The other columns are potential features (x values).

As we mentioned previously, you can perform some feature engineering to combine or derive new features. For example, let's add a new column named day to the dataframe by extracting the day component from the existing dteday column. The new column represents the day of the month, from 1 to 31.
"""

bike_data['day'] = pd.DatetimeIndex(bike_data['dteday']).day
bike_data.head(32)

numeric_features = ['temp', 'atemp', 'hum', 'windspeed']
bike_data[numeric_features + ['rentals']].describe()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt

# This ensures plots are displayed inline in the Jupyter notebook
# %matplotlib inline

# Get the label column
label = bike_data['rentals']


# Create a figure for 2 subplots (2 rows, 1 column)
fig, ax = plt.subplots(2, 1, figsize = (9,12))

# Plot the histogram
ax[0].hist(label, bins=100)
ax[0].set_ylabel('Frequency')

# Add lines for the mean, median, and mode
ax[0].axvline(label.mean(), color='magenta', linestyle='dashed', linewidth=2)
ax[0].axvline(label.median(), color='cyan', linestyle='dashed', linewidth=2)

# Plot the boxplot
ax[1].boxplot(label, vert=False)
ax[1].set_xlabel('Rentals')

# Add a title to the Figure
fig.suptitle('Rental Distribution')

# Show the figure
fig.show()

# Plot a histogram for each numeric feature
for col in numeric_features:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    feature = bike_data[col]
    feature.hist(bins=100, ax = ax)
    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)
    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)
    ax.set_title(col)
plt.show()

import numpy as np

# plot a bar plot for each categorical feature count
categorical_features = ['season','mnth','holiday','weekday','workingday','weathersit', 'day']

for col in categorical_features:
    counts = bike_data[col].value_counts().sort_index()
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    counts.plot.bar(ax = ax, color='steelblue')
    ax.set_title(col + ' counts')
    ax.set_xlabel(col)
    ax.set_ylabel("Frequency")
plt.show()

for col in numeric_features:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    feature = bike_data[col]
    label = bike_data['rentals']
    correlation = feature.corr(label)
    plt.scatter(x=feature, y=label)
    plt.xlabel(col)
    plt.ylabel('Bike Rentals')
    ax.set_title('rentals vs ' + col + '- correlation: ' + str(correlation))
plt.show()

# plot a boxplot for the label by each categorical feature
for col in categorical_features:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    bike_data.boxplot(column = 'rentals', by = col, ax = ax)
    ax.set_title('Label by ' + col)
    ax.set_ylabel("Bike Rentals")
plt.show()

# Separate features and labels
X, y = bike_data[['season','mnth', 'holiday','weekday','workingday','weathersit','temp', 'atemp', 'hum', 'windspeed']].values, bike_data['rentals'].values
print('Features:',X[:10], '\nLabels:', y[:10], sep='\n')

from sklearn.model_selection import train_test_split

# Split data 70%-30% into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

print ('Training Set: %d rows\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))

# Train the model
from sklearn.linear_model import LinearRegression

# Fit a linear regression model on the training set
model = LinearRegression().fit(X_train, y_train)
print (model)

import numpy as np

predictions = model.predict(X_test)
np.set_printoptions(suppress=True)
print('Predicted labels: ', np.round(predictions)[:10])
print('Actual labels   : ' ,y_test[:10])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

# %matplotlib inline

plt.scatter(y_test, predictions)
plt.xlabel('Actual Labels')
plt.ylabel('Predicted Labels')
plt.title('Daily Bike Share Predictions')
# overlay the regression line
z = np.polyfit(y_test, predictions, 1)
p = np.poly1d(z)
plt.plot(y_test,p(y_test), color='magenta')
plt.show()

from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, predictions)
print("MSE:", mse)

rmse = np.sqrt(mse)
print("RMSE:", rmse)

r2 = r2_score(y_test, predictions)
print("R2:", r2)